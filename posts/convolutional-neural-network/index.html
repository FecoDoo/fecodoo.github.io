<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us>
<head>
<link href=https://gmpg.org/xfn/11 rel=profile>
<meta charset=utf-8>
<meta name=robots content="nofollow">
<meta name=description content="Yao Kai">
<meta name=aplus-xplug content="NONE">
<meta name=keyword content="fecodoo,yao kai">
<meta name=generator content="Hugo 0.92.0">
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Convolutional Neural Network &#183; fecodoo</title>
<meta name=description content>
<link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/print.css media=print>
<link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/poole.css>
<link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/syntax.css>
<link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/hyde.css>
<link rel=preconnect href=https://fonts.googleapis.com>
<link rel=preconnect href=https://fonts.gstatic.com crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Blinker:wght@300;400&family=Noto+Sans+SC&display=swap" rel=stylesheet>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script>
<link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png>
<link rel="shortcut icon" href=/favicon.ico>
</head>
<body class=theme-base-08>
<aside class=sidebar>
<div class="container sidebar-sticky">
<div class=sidebar-about>
<a href=https://blog.fecodoo.ml/><h1>fecodoo</h1></a>
<p class=lead>
无不克则莫知其极，莫知其极，可以有国
</p>
</div>
<nav>
<ul class=sidebar-nav>
<li><a href=https://blog.fecodoo.ml/>Home</a> </li>
<li><a href=https://github.com/fecodoo/> Github </a></li><li><a href=https://www.linkedin.com/in/%e5%87%af-%e5%a7%9a-211242187> LinkedIn </a></li><li><a href=https://t.me/fecodoo> Telegram </a></li>
</ul>
</nav>
<p class=copyright>&copy; 2022. All rights reserved. </p>
</div>
</aside>
<main class="content container">
<div class=post>
<div class=post-title>Convolutional Neural Network</div>
<time datetime=2020-10-23T22:28:58+0800 class=post-date>Fri, Oct 23, 2020</time>
<h1 id=1-defination-of-convolution>1 Defination of convolution</h1>
<p>The mathmatical format of convolution is</p>
<ul>
<li>Continuous: $ Conv(x) = \int f(x-\alpha)t(\alpha)d\alpha $</li>
<li>Discreate: $ Conv(x)=\sum_{\alpha} f(x-\alpha)t(\alpha) $</li>
<li>Matrix: $ Conv(x) = (f * t)(x) $, where * represents the convolution process</li>
</ul>
<h1 id=2-convolution-neural-network>2 Convolution Neural Network</h1>
<h2 id=21-convolutional-layer>2.1 Convolutional layer</h2>
<h3 id=terms>Terms</h3>
<ul>
<li>W: width or height of the input matrix (输入的长度或宽度)</li>
<li>F: receptive field (感受野)</li>
<li>S: stride（步幅）</li>
<li>P: zero-padding (补零的数量)</li>
<li>K: depth of the output (深度，输出单元的深度)</li>
</ul>
<p>The output shape of after a convolution process follows:
$$\frac{S}{W-F+2P} + 1$$</p>
<h3 id=211-convolutional-kernel>2.1.1 Convolutional Kernel</h3>
<p>A convolutional kernel is a (x,x) matrix which walks through a depth slice with a predefined stride, and calculates the inner product within each step.</p>
<p><img src=http://www.elecfans.com/uploads/allimg/171115/1Z13IY6_0.gif alt=img></p>
<h3 id=212-parameter-sharing>2.1.2 Parameter Sharing</h3>
<p>We define layers at the same depth as <code>depth slice</code>. For example if the image mateix is <code>(96,96,3)</code>, it contains 3 slices with each have <code>96x96</code> pixels. The depth slices are also known as <code>channels</code>.</p>
<p>Typically, neurons (units) in a slice share the <code>same</code> weight and convolutional kernel. This is for feature reduction since repeated units can identify features without considering their positions. This trick allows us control the size of the model and it guarantees better generalization ability.</p>
<h3 id=213-relu-activation>2.1.3 ReLU Activation</h3>
<p>ReLU abbreviates for <code>rectified linear unit</code>, which can be simply represented as <code>max(0,x)</code>.</p>
<p>ReLU generates stable output since it is linear in the <code>x > 0</code> space, there is no issue of gradient disapear.</p>
<p>Another advantage of ReLu is about sparse problem. We want every neurons play it&rsquo;s role and maximize the ability of feature extraction, ReLU can amplify potential features with mean values while drop others.</p>
<h2 id=22-pooling-layer>2.2 Pooling Layer</h2>
<p>Pooling, aka downsampling, is for feature reduction which reduce the size of the feature map generated by the convolutional layer. Pooling is independent from depth slices. There are several commonly used pooling methods:</p>
<ul>
<li><strong>Max Pooling</strong>, which takes the maximum of the input matrix. This is the most common pooling method.</li>
<li><strong>Mean Pooling</strong>, which Takes the mean value of the input matrix.</li>
<li><strong>Gaussian Pooling</strong>. Borrowed from Gaussian Blur. Not commonly used.</li>
<li><strong>Trainable pooling</strong>. Trains a function that accepts a matrix as input while only output a single value. Not commonly used.</li>
</ul>
<h2 id=full-connected-layer>Full-connected Layer</h2>
<p>This is a normal hidden layer where all elements inside the input vector connect all neurons within the layer. For CNN, the activation of the full-connected layers is <code>softmax</code> in most cases.</p>
<h1 id=model-stucture>Model Stucture</h1>
<p><img src=http://pic.l2h.site/1.jpg alt=img></p>
</div>
</main>
</body>
</html>