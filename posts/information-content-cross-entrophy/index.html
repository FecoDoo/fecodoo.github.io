<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us><head><link href=https://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=robots content="nofollow"><meta name=description content="Yao Kai"><meta name=aplus-xplug content="NONE"><meta name=keyword content="fecodoo,yao kai"><meta name=generator content="Hugo 0.101.0"><meta name=viewport content="width=device-width,initial-scale=1"><title>Information Content & Cross Entrophy &#183; Y. Kai</title><meta name=description content><link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/print.css media=print><link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/poole.css><link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/syntax.css><link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/hyde.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Blinker:wght@300;400&family=Noto+Sans+SC&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/favicon.ico></head><body class=theme-base-08><aside class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><a href=https://blog.fecodoo.ml/><h1>Y. Kai</h1></a><p class=lead>无不克则莫知其极，莫知其极，可以有国</p></div><nav><ul class=sidebar-nav><li><a href=https://blog.fecodoo.ml/>Home</a></li><li><a href=https://github.com/fecodoo/>Github</a></li><li><a href=https://t.me/fecodoo>Telegram</a></li></ul></nav><p class=copyright>&copy; 2022. All rights reserved.</p></div></aside><main class="content container"><div class=post><div class=post-title>Information Content & Cross Entrophy</div><time datetime=2020-01-22T18:40:34+0200 class=post-date>Wed, Jan 22, 2020</time><h2 id=1-information-content>1 Information Content</h2><h3 id=11-defination>1.1 Defination</h3><p>In information theory, the <code>information content,</code> self-information, surprisal, or Shannon information is a basic quantity derived from the probability of a particular event occurring from a random variable.</p><p>The <code>information content</code> tells how much information is given.</p><h3 id=12-function>1.2 Function</h3><p>The function of information content need to comply with both constraints:
$$f(x) = \sum_{1}^{i} I(p_i)$$
$$x = \prod_{1}^{i} p_i$$</p><p>As a result, the function can be described as:</p><p>$$I(x) = - log_{n}(x)$$</p><p>Where $n$ is a factor describing the number of possible events in the measurement system, normally $n=2$, the metric of the information content is <strong>bit</strong>.</p><h2 id=2-entrophy>2 Entrophy</h2><p>Unlike informaiton content, entrophy measures the uncertainty of a system. Given a event $e$ with probability of $p$ to happen, the entrophy is calculated as:</p><p>$$E(e) = p \times -log_n(p) + (1-p) \times -log_n(1-p)$$</p><h2 id=3-kl-divergence>3 KL Divergence</h2><p><code>KL divergence</code>, also named relative entropy, describes the similarity between two distributions or systems, such as Poisson and Gaussian.</p><p>The <code>KL divergence</code> function can be described as:</p><p>$$KL(P||Q) = \sum_{1}^{i} p_i \times (-log_n(q_i) - \sum_{1}^{i} p_i \times (-log_n(p_i)$$</p><p>According to <a href=https://en.wikipedia.org/wiki/Gibbs%27_inequality>Gibbs&rsquo; inequality (吉布斯不等式)</a>:
$$\sum_{1}^{i} p_i \times (-log_n(p_i) \lt \sum_{1}^{i} p_i \times (-log_n(q_i)$$</p><p>Hence the KL divergence will be constantly positive. The smaller the KL divergence is, the higher similarity the two distributions have.</p><h3 id=31-cross-entrophy>3.1 Cross Entrophy</h3><p>We can notice the second part from the KL divergence function is the entrophy of distribution $Q$, where the first part calculates something much similar to the entrophy of distribution $Q$, but on the base of $P$. This is called <code>cross entrophy</code>, which is widely used in evaluating neural network models.</p><p>$$L(y_{true}||y_{pred}) = -\sum_{1}^{n} (y_{true} \times log_{2}(y_{pred}) + (1-y_{true}) \times log_{2}(1 - y_{pred}))$$</p></div></main></body></html>