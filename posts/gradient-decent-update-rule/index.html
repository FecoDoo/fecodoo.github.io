<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us>
<head>
<link href=https://gmpg.org/xfn/11 rel=profile>
<meta charset=utf-8>
<meta name=robots content="nofollow">
<meta name=description content="Yao Kai">
<meta name=aplus-xplug content="NONE">
<meta name=keyword content="fecodoo,yao kai">
<meta name=generator content="Hugo 0.92.0">
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Gradient Decent Update Rule &#183; Y. Kai</title>
<meta name=description content>
<link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/print.css media=print>
<link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/poole.css>
<link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/syntax.css>
<link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/hyde.css>
<link rel=preconnect href=https://fonts.googleapis.com>
<link rel=preconnect href=https://fonts.gstatic.com crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Blinker:wght@300;400&family=Noto+Sans+SC&display=swap" rel=stylesheet>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script>
<link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png>
<link rel="shortcut icon" href=/favicon.ico>
</head>
<body class=theme-base-08>
<aside class=sidebar>
<div class="container sidebar-sticky">
<div class=sidebar-about>
<a href=https://blog.fecodoo.ml/><h1>Y. Kai</h1></a>
<p class=lead>
无不克则莫知其极，莫知其极，可以有国
</p>
</div>
<nav>
<ul class=sidebar-nav>
<li><a href=https://blog.fecodoo.ml/>Home</a> </li>
<li><a href=https://github.com/fecodoo/> Github </a></li><li><a href=https://t.me/fecodoo> Telegram </a></li>
</ul>
</nav>
<p class=copyright>&copy; 2022. All rights reserved. </p>
</div>
</aside>
<main class="content container">
<div class=post>
<div class=post-title>Gradient Decent Update Rule</div>
<time datetime=2020-04-14T21:32:56+0800 class=post-date>Tue, Apr 14, 2020</time>
<h3 id=gradient-descent-update-rule>Gradient descent update rule</h3>
<p>$$ W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}\tag{1} $$</p>
<p>$$ b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}\tag{2} $$</p>
<blockquote>
<p>where L is the number of layers and αα is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary.</p>
<p>Note that the iterator <code>l</code> starts at <strong>0</strong> in the <code>for loop</code> while the first parameters are <code>W[1]</code> and <code>b[1]</code>.</p>
<p>You need to shift <code>l</code> to <code>l+1</code> when coding.</p>
</blockquote>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update_parameters_with_gd</span>(parameters, grads, learning_rate):
    <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>    Update parameters using one step of gradient descent
</span><span style=color:#e6db74>
</span><span style=color:#e6db74>    Arguments:
</span><span style=color:#e6db74>    parameters -- python dictionary containing your parameters to be updated:
</span><span style=color:#e6db74>                    parameters[&#39;W&#39; + str(l)] = Wl
</span><span style=color:#e6db74>                    parameters[&#39;b&#39; + str(l)] = bl
</span><span style=color:#e6db74>    grads -- python dictionary containing your gradients to update each parameters:
</span><span style=color:#e6db74>                    grads[&#39;dW&#39; + str(l)] = dWl
</span><span style=color:#e6db74>                    grads[&#39;db&#39; + str(l)] = dbl
</span><span style=color:#e6db74>    learning_rate -- the learning rate, scalar.
</span><span style=color:#e6db74>
</span><span style=color:#e6db74>    Returns:
</span><span style=color:#e6db74>    parameters -- python dictionary containing your updated parameters
</span><span style=color:#e6db74>    &#34;&#34;&#34;</span>

    L <span style=color:#f92672>=</span> len(parameters) <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span> <span style=color:#75715e># number of layers in the neural networks</span>

    <span style=color:#75715e># Update rule for each parameter</span>
    <span style=color:#66d9ef>for</span> l <span style=color:#f92672>in</span> range(L):
        <span style=color:#75715e>### START CODE HERE ### (approx. 2 lines)</span>
        parameters[<span style=color:#e6db74>&#34;W&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#34;W&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>-</span> learning_rate <span style=color:#f92672>*</span> grads[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)]
        parameters[<span style=color:#e6db74>&#34;b&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#34;b&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>-</span> learning_rate <span style=color:#f92672>*</span> grads[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)]
        <span style=color:#75715e>### END CODE HERE ###</span>

    <span style=color:#66d9ef>return</span> parameters
</code></pre></div><hr>
<h3 id=different-between-gd--sgd>Different between GD & SGD</h3>
<ul>
<li><strong>(Batch) Gradient Descent</strong></li>
</ul>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>X <span style=color:#f92672>=</span> data_input
Y <span style=color:#f92672>=</span> labels
parameters <span style=color:#f92672>=</span> initialize_parameters(layers_dims)
<span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, num_iterations):
    <span style=color:#75715e># Forward propagation</span>
    a, caches <span style=color:#f92672>=</span> forward_propagation(X, parameters)
    <span style=color:#75715e># Compute cost.</span>
    cost <span style=color:#f92672>=</span> compute_cost(a, Y)
    <span style=color:#75715e># Backward propagation.</span>
    grads <span style=color:#f92672>=</span> backward_propagation(a, caches, parameters)
    <span style=color:#75715e># Update parameters.</span>
    parameters <span style=color:#f92672>=</span> update_parameters(parameters, grads)
</code></pre></div><ul>
<li><strong>Stochastic Gradient Descent</strong>:</li>
</ul>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>X <span style=color:#f92672>=</span> data_input
Y <span style=color:#f92672>=</span> labels
parameters <span style=color:#f92672>=</span> initialize_parameters(layers_dims)
<span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, num_iterations):
    <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, m):
        <span style=color:#75715e># Forward propagation</span>
        a, caches <span style=color:#f92672>=</span> forward_propagation(X[:,j], parameters)
        <span style=color:#75715e># Compute cost</span>
        cost <span style=color:#f92672>=</span> compute_cost(a, Y[:,j])
        <span style=color:#75715e># Backward propagation</span>
        grads <span style=color:#f92672>=</span> backward_propagation(a, caches, parameters)
        <span style=color:#75715e># Update parameters.</span>
        parameters <span style=color:#f92672>=</span> update_parameters(parameters, grads)
</code></pre></div><p>In Stochastic Gradient Descent, you use only 1 training example before updating the gradients. When the training set is large, SGD can be faster. But the parameters will &ldquo;oscillate&rdquo; toward the minimum rather than converge smoothly. Here is an illustration of this:</p>
<p>{% asset_img 1550575038653.png %}</p>
<blockquote>
<p>&ldquo;+&rdquo; denotes a minimum of the cost. SGD leads to many oscillations to reach convergence. But each step is a lot faster to compute for SGD than for GD, as it uses only one training example (vs. the whole batch for GD).</p>
</blockquote>
<p><strong>Note</strong> also that implementing SGD requires 3 for-loops in total:</p>
<ol>
<li>Over the number of iterations</li>
<li>Over the $m$ training examples</li>
<li>Over the layers (to update all parameters, from $(W^{[1]},b^{[1]})$ to $(W^{[L]},b^{[L]})$)</li>
</ol>
<hr>
<h3 id=mini-batch-gradient-descent>Mini-Batch Gradient descent</h3>
<p>2 Steps</p>
<ul>
<li><strong>Shuffle</strong></li>
</ul>
<p>{% asset_img 1550575200308.png %}</p>
<ul>
<li><strong>Partition</strong>: Partition the shuffled $(X, Y)$ into <code>mini_batch_size</code></li>
</ul>
<p>{% asset_img 1550575213693.png %}</p>
<p>Note that the last mini-batch might end up smaller than <code>mini_batch_size=64</code>. Let $ \lfloor s \rfloor $ represents $ s $ rounded down to the nearest integer (this is <code>math.floor(s)</code> in Python). If the total number of examples is not a multiple of <code>mini_batch_size=64</code> then there will be
$$\lfloor \frac{m}{mini\_batch\_size}\rfloor$$
mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be
$$ m - mini\_batch\_size \times \lfloor \frac{m}{mini\_batch\_size}\rfloor $$</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>num_complete_minibatches <span style=color:#f92672>=</span> math<span style=color:#f92672>.</span>floor(m<span style=color:#f92672>/</span>mini_batch_size) <span style=color:#75715e># number of mini batches of size mini_batch_size in your partitionning</span>
    <span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, num_complete_minibatches):
        <span style=color:#75715e>### START CODE HERE ### (approx. 2 lines)</span>
        mini_batch_X <span style=color:#f92672>=</span> shuffled_X[:, k <span style=color:#f92672>*</span> mini_batch_size :(k <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> mini_batch_size]
        mini_batch_Y <span style=color:#f92672>=</span> shuffled_Y[:, k <span style=color:#f92672>*</span> mini_batch_size :(k <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> mini_batch_size]
        <span style=color:#75715e>### END CODE HERE ###</span>
        mini_batch <span style=color:#f92672>=</span> (mini_batch_X, mini_batch_Y)
        mini_batches<span style=color:#f92672>.</span>append(mini_batch)

    <span style=color:#75715e># Handling the end case (last mini-batch &lt; mini_batch_size)</span>
    <span style=color:#66d9ef>if</span> m <span style=color:#f92672>%</span> mini_batch_size <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>:
        <span style=color:#75715e>### START CODE HERE ### (approx. 2 lines)</span>
        mini_batch_X <span style=color:#f92672>=</span> shuffled_X[:,num_complete_minibatches <span style=color:#f92672>*</span> mini_batch_size:]
        mini_batch_Y <span style=color:#f92672>=</span> shuffled_Y[:,num_complete_minibatches <span style=color:#f92672>*</span> mini_batch_size:]
        <span style=color:#75715e>### END CODE HERE ###</span>
        mini_batch <span style=color:#f92672>=</span> (mini_batch_X, mini_batch_Y)
        mini_batches<span style=color:#f92672>.</span>append(mini_batch)
</code></pre></div><hr>
<h3 id=momentum>Momentum</h3>
<ul>
<li>Initialize</li>
</ul>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>for</span> l <span style=color:#f92672>in</span> range(L):
        v[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros_like(parameters[<span style=color:#e6db74>&#34;W&#34;</span><span style=color:#f92672>+</span>str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)])
        v[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros_like(parameters[<span style=color:#e6db74>&#34;b&#34;</span><span style=color:#f92672>+</span>str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)])
</code></pre></div><ul>
<li>Update</li>
</ul>
<p>$$ v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} $$
$$W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}} $$</p>
<hr>
<p>$$ v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} $$
$$ b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}} $$</p>
<p>where <code>l</code> is the number of layers, $\beta$ is the momentum and $\alpha$ is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary. Note that the iterator <code>l</code> starts at 0 in the <code>for</code> loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that&rsquo;s a &ldquo;one&rdquo; on the superscript). So you will need to shift <code>l</code> to <code>l+1</code> when coding.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>for</span> l <span style=color:#f92672>in</span> range(L):
        <span style=color:#75715e># compute velocities</span>
        v[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> beta <span style=color:#f92672>*</span> v[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta) <span style=color:#f92672>*</span> grads[<span style=color:#e6db74>&#39;dW&#39;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)]
        v[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> beta <span style=color:#f92672>*</span> v[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta) <span style=color:#f92672>*</span> grads[<span style=color:#e6db74>&#39;db&#39;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)]
        <span style=color:#75715e># update parameters</span>
        parameters[<span style=color:#e6db74>&#34;W&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#34;W&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>-</span> learning_rate <span style=color:#f92672>*</span> v[<span style=color:#e6db74>&#34;dW&#34;</span><span style=color:#f92672>+</span>str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)]
        parameters[<span style=color:#e6db74>&#34;b&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#34;b&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>-</span> learning_rate <span style=color:#f92672>*</span> v[<span style=color:#e6db74>&#34;db&#34;</span><span style=color:#f92672>+</span>str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)]
</code></pre></div><hr>
<h3 id=adam>Adam</h3>
<h4 id=synopsis>Synopsis</h4>
<ol>
<li>
<p>It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction).</p>
</li>
<li>
<p>It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction).</p>
</li>
<li>
<p>It updates parameters in a direction based on combining information from &ldquo;1&rdquo; and &ldquo;2&rdquo;.</p>
</li>
</ol>
<h4 id=update>Update</h4>
<p>$$ v_{dW} = \beta_1 v_{dW} + (1 - \beta_1) dW $$
$$ v_{db} = \beta_1 v_{db} + (1 - \beta_1) db $$
$$ s_{dW} = \beta_2 s_{dW} + (1 - \beta_2) (dW)^2 $$
$$ s_{db} = \beta_2 s_{db} + (1 - \beta_2) (db)^2 $$</p>
<hr>
<p>$$ v^{corrected}{dW} = \frac{v_{dW}}{1 - (\beta<em>1)^t} $$
$$ v^{corrected}</em> {db} = \frac{v*{db}}{1 - (\beta_1)^t} $$
$$ s^{corrected}* {dW} = \frac{s*{dW}}{1 - (\beta_1)^t} $$
$$ s^{corrected}* {db} = \frac{s_{db}}{1 - (\beta_2)^t} $$</p>
<hr>
<p>$$ W = W - \alpha \frac{v_{dW}^{corrected}}{\sqrt{s_{dW}^{corrected} + \varepsilon}} $$
$$ b = b - \alpha \frac{v_{db}^{corrected}}{\sqrt{s_{db}^{corrected} + \varepsilon}} $$</p>
<hr>
<p>where:</p>
<ul>
<li>t counts the number of steps taken of Adam</li>
<li>L is the number of layers</li>
<li>$\beta_1$ and $\beta_2$ are hyperparameters that control the two exponentially weighted averages.</li>
<li>$ \alpha $ is the learning rate</li>
<li>$ \varepsilon $ is a very small number to avoid dividing by zero</li>
</ul>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>for</span> l <span style=color:#f92672>in</span> range(L):
        <span style=color:#75715e># Moving average of the gradients. Inputs: &#34;v, grads, beta1&#34;. Output: &#34;v&#34;.</span>
        v[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> beta1 <span style=color:#f92672>*</span> v[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta1) <span style=color:#f92672>*</span> grads[<span style=color:#e6db74>&#39;dW&#39;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)]
        v[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> beta1 <span style=color:#f92672>*</span> v[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta1) <span style=color:#f92672>*</span> grads[<span style=color:#e6db74>&#39;db&#39;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)]

        <span style=color:#75715e># Compute bias-corrected first moment estimate. Inputs: &#34;v, beta1, t&#34;. Output: &#34;v_corrected&#34;.</span>
        v_corrected[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> v[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)]<span style=color:#f92672>/</span>(<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>power(beta1, t))
        v_corrected[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> v[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)]<span style=color:#f92672>/</span>(<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>power(beta1, t))

        <span style=color:#75715e># Moving average of the squared gradients. Inputs: &#34;s, grads, beta2&#34;. Output: &#34;s&#34;.</span>
        s[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> beta2 <span style=color:#f92672>*</span> s[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta2) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>power(grads[<span style=color:#e6db74>&#39;dW&#39;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)], <span style=color:#ae81ff>2</span>)
        s[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> beta2 <span style=color:#f92672>*</span> s[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta2) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>power(grads[<span style=color:#e6db74>&#39;db&#39;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)], <span style=color:#ae81ff>2</span>)

        <span style=color:#75715e># Compute bias-corrected second raw moment estimate. Inputs: &#34;s, beta2, t&#34;. Output: &#34;s_corrected&#34;.</span>
        s_corrected[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> s[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)]<span style=color:#f92672>/</span>(<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>power(beta2, t))
        s_corrected[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> s[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)]<span style=color:#f92672>/</span>(<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>power(beta2, t))

        <span style=color:#75715e># Update parameters. Inputs: &#34;parameters, learning_rate, v_corrected, s_corrected, epsilon&#34;. Output: &#34;parameters&#34;.</span>
        parameters[<span style=color:#e6db74>&#34;W&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#34;W&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>-</span> learning_rate <span style=color:#f92672>*</span> v_corrected[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>sqrt(s_corrected[<span style=color:#e6db74>&#34;dW&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>+</span> epsilon)
        parameters[<span style=color:#e6db74>&#34;b&#34;</span> <span style=color:#f92672>+</span> str(l<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)] <span style=color:#f92672>=</span> parameters[<span style=color:#e6db74>&#34;b&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>-</span> learning_rate <span style=color:#f92672>*</span> v_corrected[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>sqrt(s_corrected[<span style=color:#e6db74>&#34;db&#34;</span> <span style=color:#f92672>+</span> str(l <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)] <span style=color:#f92672>+</span> epsilon)
</code></pre></div>
</div>
</main>
</body>
</html>