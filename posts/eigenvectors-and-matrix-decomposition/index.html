<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us><head><link href=https://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=robots content="nofollow"><meta name=description content="Yao Kai"><meta name=aplus-xplug content="NONE"><meta name=keyword content="fecodoo,yao kai"><meta name=generator content="Hugo 0.101.0"><meta name=viewport content="width=device-width,initial-scale=1"><title>Eigenvectors and Matrix Decomposition &#183; Y. Kai</title><meta name=description content><link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/print.css media=print><link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/poole.css><link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/syntax.css><link type=text/css rel=stylesheet href=https://blog.fecodoo.ml/css/hyde.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Blinker:wght@300;400&family=Noto+Sans+SC&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/favicon.ico></head><body class=theme-base-08><aside class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><a href=https://blog.fecodoo.ml/><h1>Y. Kai</h1></a><p class=lead>无不克则莫知其极，莫知其极，可以有国</p></div><nav><ul class=sidebar-nav><li><a href=https://blog.fecodoo.ml/>Home</a></li><li><a href=https://github.com/fecodoo/>Github</a></li><li><a href=https://t.me/fecodoo>Telegram</a></li></ul></nav><p class=copyright>&copy; 2022. All rights reserved.</p></div></aside><main class="content container"><div class=post><div class=post-title>Eigenvectors and Matrix Decomposition</div><time datetime=2018-06-16T16:41:57+0800 class=post-date>Sat, Jun 16, 2018</time><h1 id=basic-concepts>Basic concepts</h1><h2 id=linear-transformation>Linear Transformation</h2><p>A matrix can be seen as a linear transforamtion, for which the most important factors are the speed and direction:</p><ul><li>Eigenvalue is the velocity</li><li>Eigenvector is the direction</li></ul><h2 id=rank>Rank</h2><p>The rank of the matrix represents the dimension. It also indicates the number of the eigenvectors (linear independent base vectors) of the transformation.</p><hr><h1 id=eigenvectors-and-eigenvalue>Eigenvectors and Eigenvalue</h1><p>Matrix $A$ is a linear transformation, and it can be represents as follows:</p><p>$$Au = \lambda u$$</p><p>$u$ is one of the eigenvectors of matrix $A$. The equation shows that the linear transformation of $A$ only scales the eigenvector $u$ by $\lambda$ times.</p><p>Eigenvectors can be seen as base vectors in a coordinate system during the linear transformation. The linear transformation equals to a scaling operation on the direction of eigenvectors where the scaling factors are eigenvalues.</p><hr><h1 id=feature-decomposition>Feature Decomposition</h1><p>If a $n\times n$ matrix $A$ has $n$ eigenvectors (the rank of $A$ equals to $n$), then matrix $A$ are decomposable.
$$A = Q^{-1}\Lambda Q$$
$Q$ is a square matrix of $n$ eigenvectors, where the $i^{th}$ column is the eigenvector $e_i$.
$\Lambda$ is a diagonal matrix where values on the diagonal are eigenvales (scaling values).</p></div></main></body></html>