---
title: "AI面试相关内容"
date: 2020-02-22T13:21:37+08:00
draft: true
---

## 什么是线性回归

通过寻找一组基于所有输入维度的线性组合，来获得一个能够最精确描述给定样本的函数

## 最小二乘法获得的最佳w

$w = (X^TX)^{-1}X^Ty$

如果特征共线性，则$(X^TX)^{-1}$不会是满秩，没有唯一解

可以通过添加正则化l1l2或者PCA降维

## 正则

`L1正则化是指权值向量`$w$中各个元素的绝对值之和，通常表示为$ \lambda ||w|| $

`L2正则化`是指权值向量$w$中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为$\frac{\lambda}{2}||w||^2$

一般都会在正则化项之前添加一个系数$\lambda$

`L1正则化`可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
`L2正则化`可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合

## Activation
常用的非线性激活函数

### Sigmoid

一般就用在**二分类**最后激活层上

导数范围**0-0.25**,有**软饱和**现象，如果用在网络中，由于反向传播会乘上导数，因此会造成浅层神经元**梯度消失**，同时均值也不再为0

### Tahn

改进sigmoid，分布范围为**-1 ~ 1**，很大程度上避免了梯度消失的现象，并且保持均值为0

### Relu
最简单容易计算的非线性激活函数，在0时不可导，**-1 ~ 0** 与 **0 ~ $\infty$** 均为线性

## 损失函数
评估预测值与实际值间的差距

### MSE
最小平方误差

$$l = \frac{1}{N}\sum_{i=0}^{N}(y-y_i)^2$$
**交叉熵是用来评估当前训练得到的概率分布与真实分布的差异情况。
它刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。**

## 技巧

### 归一化层（Batch Normalization）
虽然我们对输入数据进行了归一化处理，但是输入数据经过 σ(WX+b) 这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着