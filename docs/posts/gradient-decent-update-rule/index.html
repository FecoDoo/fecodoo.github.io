


<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="robots" content="nofollow" />
  <meta name="description" content="Yao Kai" />
  <meta name="aplus-xplug" content="NONE">
  <meta name="keyword" content="fecodoo,yao kai"/>

  <meta name="generator" content="Hugo 0.81.0" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Gradient Decent Update Rule &middot; Yao Kai</title>
  <meta name="description" content="" />

  
  <link type="text/css" rel="stylesheet" href="https://fecodoo.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://fecodoo.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://fecodoo.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://fecodoo.github.io/css/hyde.css">
  
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Courier+Prime&family=Blinker:wght@300&family=Ubuntu+Mono&family=ZCOOL+XiaoWei&display=swap" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  
</head>

  <body class="theme-base-08 ">
  <aside class="sidebar">
    <div class="container sidebar-sticky">
      <div class="sidebar-about">
        <a href="https://fecodoo.github.io/"><h1>Yao Kai</h1></a>
        <p class="lead">
         无不克则莫知其极，莫知其极，可以有国 
        </p>
      </div>
  
      <nav>
        <ul class="sidebar-nav">
          <li><a href="https://fecodoo.github.io/">Home</a> </li>
          <li><a href="https://github.com/fecodoo/"> Github </a></li><li><a href="https://www.linkedin.com/in/%e5%87%af-%e5%a7%9a-211242187"> LinkedIn </a></li><li><a href="https://t.me/fecodoo"> Telegram </a></li>
        </ul>
      </nav>
  
      <p class="copyright">&copy; 2021. All rights reserved. </p>
    </div>
  </aside>
  
    <main class="content container">
    <div class="post">
  <div class='post-title'>Gradient Decent Update Rule</div>
  <time datetime=2020-04-14T21:32:56&#43;0800 class="post-date">Tue, Apr 14, 2020</time>
  <h3 id="gradient-descent-update-rule">Gradient descent update rule</h3>
<p>$$ W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}\tag{1} $$</p>
<p>$$ b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}\tag{2} $$</p>
<blockquote>
<p>where L is the number of layers and αα is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary.</p>
<p>Note that the iterator <code>l</code> starts at <strong>0</strong> in the <code>for loop</code> while the first parameters are <code>W[1]</code> and <code>b[1]</code>.</p>
<p>You need to shift <code>l</code> to <code>l+1</code> when coding.</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update_parameters_with_gd</span>(parameters, grads, learning_rate):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Update parameters using one step of gradient descent
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    Arguments:
</span><span style="color:#e6db74">    parameters -- python dictionary containing your parameters to be updated:
</span><span style="color:#e6db74">                    parameters[&#39;W&#39; + str(l)] = Wl
</span><span style="color:#e6db74">                    parameters[&#39;b&#39; + str(l)] = bl
</span><span style="color:#e6db74">    grads -- python dictionary containing your gradients to update each parameters:
</span><span style="color:#e6db74">                    grads[&#39;dW&#39; + str(l)] = dWl
</span><span style="color:#e6db74">                    grads[&#39;db&#39; + str(l)] = dbl
</span><span style="color:#e6db74">    learning_rate -- the learning rate, scalar.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">    parameters -- python dictionary containing your updated parameters
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>

    L <span style="color:#f92672">=</span> len(parameters) <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span> <span style="color:#75715e"># number of layers in the neural networks</span>

    <span style="color:#75715e"># Update rule for each parameter</span>
    <span style="color:#66d9ef">for</span> l <span style="color:#f92672">in</span> range(L):
        <span style="color:#75715e">### START CODE HERE ### (approx. 2 lines)</span>
        parameters[<span style="color:#e6db74">&#34;W&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> parameters[<span style="color:#e6db74">&#34;W&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> grads[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)]
        parameters[<span style="color:#e6db74">&#34;b&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> parameters[<span style="color:#e6db74">&#34;b&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> grads[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)]
        <span style="color:#75715e">### END CODE HERE ###</span>

    <span style="color:#66d9ef">return</span> parameters
</code></pre></div><hr>
<h3 id="different-between-gd--sgd">Different between GD &amp; SGD</h3>
<ul>
<li><strong>(Batch) Gradient Descent</strong></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X <span style="color:#f92672">=</span> data_input
Y <span style="color:#f92672">=</span> labels
parameters <span style="color:#f92672">=</span> initialize_parameters(layers_dims)
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, num_iterations):
    <span style="color:#75715e"># Forward propagation</span>
    a, caches <span style="color:#f92672">=</span> forward_propagation(X, parameters)
    <span style="color:#75715e"># Compute cost.</span>
    cost <span style="color:#f92672">=</span> compute_cost(a, Y)
    <span style="color:#75715e"># Backward propagation.</span>
    grads <span style="color:#f92672">=</span> backward_propagation(a, caches, parameters)
    <span style="color:#75715e"># Update parameters.</span>
    parameters <span style="color:#f92672">=</span> update_parameters(parameters, grads)
</code></pre></div><ul>
<li><strong>Stochastic Gradient Descent</strong>:</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X <span style="color:#f92672">=</span> data_input
Y <span style="color:#f92672">=</span> labels
parameters <span style="color:#f92672">=</span> initialize_parameters(layers_dims)
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, num_iterations):
    <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, m):
        <span style="color:#75715e"># Forward propagation</span>
        a, caches <span style="color:#f92672">=</span> forward_propagation(X[:,j], parameters)
        <span style="color:#75715e"># Compute cost</span>
        cost <span style="color:#f92672">=</span> compute_cost(a, Y[:,j])
        <span style="color:#75715e"># Backward propagation</span>
        grads <span style="color:#f92672">=</span> backward_propagation(a, caches, parameters)
        <span style="color:#75715e"># Update parameters.</span>
        parameters <span style="color:#f92672">=</span> update_parameters(parameters, grads)
</code></pre></div><p>In Stochastic Gradient Descent, you use only 1 training example before updating the gradients. When the training set is large, SGD can be faster. But the parameters will &ldquo;oscillate&rdquo; toward the minimum rather than converge smoothly. Here is an illustration of this:</p>
<p>{% asset_img 1550575038653.png %}</p>
<blockquote>
<p>&ldquo;+&rdquo; denotes a minimum of the cost. SGD leads to many oscillations to reach convergence. But each step is a lot faster to compute for SGD than for GD, as it uses only one training example (vs. the whole batch for GD).</p>
</blockquote>
<p><strong>Note</strong> also that implementing SGD requires 3 for-loops in total:</p>
<ol>
<li>Over the number of iterations</li>
<li>Over the $m$ training examples</li>
<li>Over the layers (to update all parameters, from $(W^{[1]},b^{[1]})$ to $(W^{[L]},b^{[L]})$)</li>
</ol>
<hr>
<h3 id="mini-batch-gradient-descent">Mini-Batch Gradient descent</h3>
<p>2 Steps</p>
<ul>
<li><strong>Shuffle</strong></li>
</ul>
<p>{% asset_img 1550575200308.png %}</p>
<ul>
<li><strong>Partition</strong>: Partition the shuffled $(X, Y)$ into <code>mini_batch_size</code></li>
</ul>
<p>{% asset_img 1550575213693.png %}</p>
<p>Note that the last mini-batch might end up smaller than <code>mini_batch_size=64</code>. Let $ \lfloor s \rfloor $ represents $ s $ rounded down to the nearest integer (this is <code>math.floor(s)</code> in Python). If the total number of examples is not a multiple of <code>mini_batch_size=64</code> then there will be
$$\lfloor \frac{m}{mini\_batch\_size}\rfloor$$
mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be
$$ m - mini\_batch\_size \times \lfloor \frac{m}{mini\_batch\_size}\rfloor $$</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">num_complete_minibatches <span style="color:#f92672">=</span> math<span style="color:#f92672">.</span>floor(m<span style="color:#f92672">/</span>mini_batch_size) <span style="color:#75715e"># number of mini batches of size mini_batch_size in your partitionning</span>
    <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, num_complete_minibatches):
        <span style="color:#75715e">### START CODE HERE ### (approx. 2 lines)</span>
        mini_batch_X <span style="color:#f92672">=</span> shuffled_X[:, k <span style="color:#f92672">*</span> mini_batch_size :(k <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> mini_batch_size]
        mini_batch_Y <span style="color:#f92672">=</span> shuffled_Y[:, k <span style="color:#f92672">*</span> mini_batch_size :(k <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> mini_batch_size]
        <span style="color:#75715e">### END CODE HERE ###</span>
        mini_batch <span style="color:#f92672">=</span> (mini_batch_X, mini_batch_Y)
        mini_batches<span style="color:#f92672">.</span>append(mini_batch)

    <span style="color:#75715e"># Handling the end case (last mini-batch &lt; mini_batch_size)</span>
    <span style="color:#66d9ef">if</span> m <span style="color:#f92672">%</span> mini_batch_size <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#75715e">### START CODE HERE ### (approx. 2 lines)</span>
        mini_batch_X <span style="color:#f92672">=</span> shuffled_X[:,num_complete_minibatches <span style="color:#f92672">*</span> mini_batch_size:]
        mini_batch_Y <span style="color:#f92672">=</span> shuffled_Y[:,num_complete_minibatches <span style="color:#f92672">*</span> mini_batch_size:]
        <span style="color:#75715e">### END CODE HERE ###</span>
        mini_batch <span style="color:#f92672">=</span> (mini_batch_X, mini_batch_Y)
        mini_batches<span style="color:#f92672">.</span>append(mini_batch)
</code></pre></div><hr>
<h3 id="momentum">Momentum</h3>
<ul>
<li>Initialize</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> l <span style="color:#f92672">in</span> range(L):
        v[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros_like(parameters[<span style="color:#e6db74">&#34;W&#34;</span><span style="color:#f92672">+</span>str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)])
        v[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros_like(parameters[<span style="color:#e6db74">&#34;b&#34;</span><span style="color:#f92672">+</span>str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)])
</code></pre></div><ul>
<li>Update</li>
</ul>
<p>$$ v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} $$
$$W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}} $$</p>
<hr>
<p>$$ v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} $$ 
$$ b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}} $$</p>
<p>where <code>l</code> is the number of layers, $\beta$ is the momentum and $\alpha$ is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary. Note that the iterator <code>l</code> starts at 0 in the <code>for</code> loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that&rsquo;s a &ldquo;one&rdquo; on the superscript). So you will need to shift <code>l</code> to <code>l+1</code> when coding.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> l <span style="color:#f92672">in</span> range(L):
        <span style="color:#75715e"># compute velocities</span>
        v[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> beta <span style="color:#f92672">*</span> v[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta) <span style="color:#f92672">*</span> grads[<span style="color:#e6db74">&#39;dW&#39;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)]
        v[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> beta <span style="color:#f92672">*</span> v[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta) <span style="color:#f92672">*</span> grads[<span style="color:#e6db74">&#39;db&#39;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)]
        <span style="color:#75715e"># update parameters</span>
        parameters[<span style="color:#e6db74">&#34;W&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> parameters[<span style="color:#e6db74">&#34;W&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> v[<span style="color:#e6db74">&#34;dW&#34;</span><span style="color:#f92672">+</span>str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)]
        parameters[<span style="color:#e6db74">&#34;b&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> parameters[<span style="color:#e6db74">&#34;b&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> v[<span style="color:#e6db74">&#34;db&#34;</span><span style="color:#f92672">+</span>str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)]
</code></pre></div><hr>
<h3 id="adam">Adam</h3>
<h4 id="synopsis">Synopsis</h4>
<ol>
<li>
<p>It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction).</p>
</li>
<li>
<p>It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction).</p>
</li>
<li>
<p>It updates parameters in a direction based on combining information from &ldquo;1&rdquo; and &ldquo;2&rdquo;.</p>
</li>
</ol>
<h4 id="update">Update</h4>
<p>$$ v_{dW} = \beta_1 v_{dW} + (1 - \beta_1) dW  $$
$$ v_{db} = \beta_1 v_{db} + (1 - \beta_1) db $$
$$ s_{dW} = \beta_2 s_{dW} + (1 - \beta_2) (dW)^2 $$
$$ s_{db} = \beta_2 s_{db} + (1 - \beta_2) (db)^2 $$</p>
<hr>
<p>$$ v^{corrected}{dW} = \frac{v_{dW}}{1 - (\beta*1)^t} $$
$$ v^{corrected}* {db} = \frac{v*{db}}{1 - (\beta_1)^t} $$
$$ s^{corrected}* {dW} = \frac{s*{dW}}{1 - (\beta_1)^t} $$
$$ s^{corrected}* {db} = \frac{s_{db}}{1 - (\beta_2)^t} $$</p>
<hr>
<p>$$ W = W - \alpha \frac{v_{dW}^{corrected}}{\sqrt{s_{dW}^{corrected} + \varepsilon}} $$
$$ b = b - \alpha \frac{v_{db}^{corrected}}{\sqrt{s_{db}^{corrected} + \varepsilon}} $$</p>
<hr>
<p>where:</p>
<ul>
<li>t counts the number of steps taken of Adam</li>
<li>L is the number of layers</li>
<li>$\beta_1$ and $\beta_2$ are hyperparameters that control the two exponentially weighted averages.</li>
<li>$ \alpha $ is the learning rate</li>
<li>$ \varepsilon $ is a very small number to avoid dividing by zero</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> l <span style="color:#f92672">in</span> range(L):
        <span style="color:#75715e"># Moving average of the gradients. Inputs: &#34;v, grads, beta1&#34;. Output: &#34;v&#34;.</span>
        v[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> beta1 <span style="color:#f92672">*</span> v[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta1) <span style="color:#f92672">*</span> grads[<span style="color:#e6db74">&#39;dW&#39;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)]
        v[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> beta1 <span style="color:#f92672">*</span> v[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta1) <span style="color:#f92672">*</span> grads[<span style="color:#e6db74">&#39;db&#39;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)]

        <span style="color:#75715e"># Compute bias-corrected first moment estimate. Inputs: &#34;v, beta1, t&#34;. Output: &#34;v_corrected&#34;.</span>
        v_corrected[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> v[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)]<span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>power(beta1, t))
        v_corrected[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> v[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)]<span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>power(beta1, t))

        <span style="color:#75715e"># Moving average of the squared gradients. Inputs: &#34;s, grads, beta2&#34;. Output: &#34;s&#34;.</span>
        s[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> beta2 <span style="color:#f92672">*</span> s[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta2) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>power(grads[<span style="color:#e6db74">&#39;dW&#39;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)], <span style="color:#ae81ff">2</span>)
        s[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> beta2 <span style="color:#f92672">*</span> s[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta2) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>power(grads[<span style="color:#e6db74">&#39;db&#39;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)], <span style="color:#ae81ff">2</span>)

        <span style="color:#75715e"># Compute bias-corrected second raw moment estimate. Inputs: &#34;s, beta2, t&#34;. Output: &#34;s_corrected&#34;.</span>
        s_corrected[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> s[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)]<span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>power(beta2, t))
        s_corrected[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> s[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)]<span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>power(beta2, t))

        <span style="color:#75715e"># Update parameters. Inputs: &#34;parameters, learning_rate, v_corrected, s_corrected, epsilon&#34;. Output: &#34;parameters&#34;.</span>
        parameters[<span style="color:#e6db74">&#34;W&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> parameters[<span style="color:#e6db74">&#34;W&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> v_corrected[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sqrt(s_corrected[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">+</span> epsilon)
        parameters[<span style="color:#e6db74">&#34;b&#34;</span> <span style="color:#f92672">+</span> str(l<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)] <span style="color:#f92672">=</span> parameters[<span style="color:#e6db74">&#34;b&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> v_corrected[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sqrt(s_corrected[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)] <span style="color:#f92672">+</span> epsilon)
</code></pre></div>
</div>

    </main>

    
      
    
  </body>
</html>
