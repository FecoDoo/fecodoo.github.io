<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yao Kai</title>
    <link>https://fecodoo.github.io/</link>
    <description>Recent content on Yao Kai</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Mar 2021 19:10:53 +0800</lastBuildDate><atom:link href="https://fecodoo.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Machine Learning With LCMS</title>
      <link>https://fecodoo.github.io/posts/machine-learning-for-small-molecule-identification/</link>
      <pubDate>Thu, 18 Mar 2021 19:10:53 +0800</pubDate>
      
      <guid>https://fecodoo.github.io/posts/machine-learning-for-small-molecule-identification/</guid>
      <description>Learning Diary 2 1 Abstract In metabolite analysis, we are trying to study and annotate different molecules by their features. However, small molecules typically contain similar structures and the space of potential structures is tremendous (over 600 million). The amount of unique spectra covers only a small fraction of the entire database.
With the input out of the LC-MS^2 process, the machine learning algorithm runs like a search engine that assigns ranked scores to predicted structures of high similarities.</description>
    </item>
    
    <item>
      <title>HOFM</title>
      <link>https://fecodoo.github.io/posts/hofm/</link>
      <pubDate>Fri, 12 Mar 2021 00:40:22 +0800</pubDate>
      
      <guid>https://fecodoo.github.io/posts/hofm/</guid>
      <description>Abstract According to practical experience and medical treatment records, it shows that combinations of multiple drugs may positively effect the curing process regarding to certain disease. People are investigating about the effectiveness of the drug combinations with certian dosage against senarios when used respectively. However, the number of potencial combinations may grow exponentially when try to add more medicines into consideration, this refers as combinatorial explosion. Hence, we need to prioritize and reduce ranks for the search for combinations, whereas certrain machine learning methods can contribute.</description>
    </item>
    
    <item>
      <title>Convolutional Neural Network</title>
      <link>https://fecodoo.github.io/posts/convolutional-neural-network/</link>
      <pubDate>Tue, 23 Feb 2021 22:28:58 +0800</pubDate>
      
      <guid>https://fecodoo.github.io/posts/convolutional-neural-network/</guid>
      <description>1 Defination of convolution The mathmatical format of convolution is
 Continuous: $ Conv(x) = \int f(x-\alpha)t(\alpha)d\alpha $ Discreate: $ Conv(x)=\sum_{\alpha} f(x-\alpha)t(\alpha) $ Matrix: $Conv(x) = (f * t)(x)$, where $*$ represents the convolution process  2 Convolution Neural Network 2.1 Convolutional layer Terms  W: width or height of the input matrix (输入的长度或宽度) F: receptive field (感受野) S: stride（步幅） P: zero-padding (补零的数量) K: depth of the output (深度，输出单元的深度)  The output shape of after a convolution process follows: $$\frac{S}{W-F+2P} + 1$$</description>
    </item>
    
    <item>
      <title>Bidirectional LSTM</title>
      <link>https://fecodoo.github.io/posts/bidirectional-lstm/</link>
      <pubDate>Sun, 08 Nov 2020 21:24:50 +0800</pubDate>
      
      <guid>https://fecodoo.github.io/posts/bidirectional-lstm/</guid>
      <description>为什么用双向 LSTM？ 单向的 RNN，是根据前面的信息推出后面的，但有时候只看前面的词是不够的， 例如，
我今天不舒服，我打算____一天。
只根据‘不舒服‘，可能推出我打算‘去医院‘，‘睡觉‘，‘请假‘等等，但如果加上后面的‘一天‘，能选择的范围就变小了，‘去医院‘这种就不能选了，而‘请假‘‘休息‘之类的被选择概率就会更大。
什么是双向 LSTM？ 双向卷积神经网络的隐藏层要保存两个值， A 参与正向计算， A&#39; 参与反向计算。 最终的输出值 y 取决于 A 和 A&#39;：
结构 我们已经看到了 Encoder-Decoder LSTM 的介绍中讨论的 LSTMs 输入序列的顺序的好处。
 我们对源句子中颠倒词的改进程度感到惊讶。
— Sequence to Sequence Learning with Neural Networks, 2014.
 Bidirectional LSTMs 专注于通过输入和输出时间步长在向前和向后两个方向上获得最大的输入序列的问题。在实践中，该架构涉及复制网络中的第一个递归层，使得现在有两个并排的层，然后提供输入序列，作为输入到第一层并且提供输入序列到第二层的反向副本。这种方法是在不久前发展起来的一种用于改善循环神经网络（RNNs）性能的一般方法。
 为了克服常规 RNN 的局限性&amp;hellip;我们提出了一个双向递归神经网络（BRNN），可以使用所有可用的输入信息在过去和未来的特定时间帧进行训练。&amp;hellip;我们的方法是将一个规则的 RNN 状态神经元分裂成两个部分，一部分负责正时间方向（正向状态），另外一个部分负责负时间方向（后向状态）。
— Bidirectional Recurrent Neural Networks, 1997.
 该方法以及被应用于 LSTM 循环神经网络。向前和向后提供整个序列是基于假设整个序列是可用的假设的。在使用矢量化输入时，在这个实践中通常是一个要求。然而，它可能会引起哲学上的关注，其中理想的时间步长是按顺序和及时（just-in-time）提供的。在语音识别领域中，双向地提供输入序列是合理的，因为有证据表明，在人类中，整个话语的上下文被用来解释所说的话而不是一个线性解释。
 &amp;hellip;依赖于乍一看是违反因果关系的未来知识。我们如何才能理解我们所听到的关于海没有说过的话呢？然而，人类的厅总就是这样做的。在未来的语境中，声音、词语乃至于整个句子都是毫无意义的。我们必须记住的是，任务之间的区别是真的在线的——在每个输入之后需要一个输出，以及在某些输入段的末尾只需要输出。
— Framewise Phoneme Classiﬁcation with Bidirectional LSTM and Other Neural Network Architectures, 2005.</description>
    </item>
    
    <item>
      <title>Gradient Decent Update Rule</title>
      <link>https://fecodoo.github.io/posts/gradient-decent-update-rule/</link>
      <pubDate>Tue, 14 Apr 2020 21:32:56 +0800</pubDate>
      
      <guid>https://fecodoo.github.io/posts/gradient-decent-update-rule/</guid>
      <description>Gradient descent update rule $$ W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}\tag{1} $$
$$ b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}\tag{2} $$
 where L is the number of layers and αα is the learning rate. All parameters should be stored in the parameters dictionary.
Note that the iterator l starts at 0 in the for loop while the first parameters are W[1] and b[1].
You need to shift l to l+1 when coding.</description>
    </item>
    
    <item>
      <title>Eigenvectors and Matrix Decomposition</title>
      <link>https://fecodoo.github.io/posts/eigenvectors-and-matrix-decomposition/</link>
      <pubDate>Sat, 16 Jun 2018 16:41:57 +0800</pubDate>
      
      <guid>https://fecodoo.github.io/posts/eigenvectors-and-matrix-decomposition/</guid>
      <description>Basic concepts Linear Transformation A matrix can be seen as a linear transforamtion, for which the most important factors are the speed and direction:
 Eigenvalue is the velocity Eigenvector is the direction  Rank The rank of the matrix represents the dimension. It also indicates the number of the eigenvectors (linear independent base vectors) of the transformation.
 Eigenvectors and Eigenvalue Matrix $A$ is a linear transformation, and it can be represents as follows:</description>
    </item>
    
    <item>
      <title>About me</title>
      <link>https://fecodoo.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://fecodoo.github.io/about/</guid>
      <description>about</description>
    </item>
    
    <item>
      <title>Archive</title>
      <link>https://fecodoo.github.io/archives/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://fecodoo.github.io/archives/</guid>
      <description>archives</description>
    </item>
    
    <item>
      <title>PageRank&amp;SimRank</title>
      <link>https://fecodoo.github.io/posts/pageranksimrank/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://fecodoo.github.io/posts/pageranksimrank/</guid>
      <description>Random Walk Given a graph, a random walk is an iterative process that starts from a random vertex, and at each step, either follows a random outgoing edge of the current vertex or jumps to a random vertex. The jump part is important because some vertices may not have any outgoing edges so a walk will terminate at those places without jumping to another vertex.
Page Rank (PR) measures stationary distribution of one specific kind of random walk that starts from a random vertex and in each iteration, with a predefined probability p, jumps to a random vertex, and with probability1-p follows a random outgoing edge of the current vertex.</description>
    </item>
    
  </channel>
</rss>
